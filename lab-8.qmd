---
title: "Lab 8"
subtitle: "Cheese Gromit!"
editor: source
---

> **Goal:** Scrape information from <https://www.cheese.com> to obtain a dataset of characteristics about different cheeses, and gain deeper insight into your coding process. ðŸª¤

**Part 1:** Locate and examine the `robots.txt` file for this website. Summarize what you learn from it.

The robots.txt file for the cheese.com website showed us that any user is authorized to scrape the data from this site. The other other piece of information provided was the sitemap at <https://www.cheese.com/sitemap.xml>. Basically, there are no restrictions for webscraping on this cheese website.

**Part 2:** Learn about the `html_attr()` function from `rvest`. Describe how this function works with a small example.

The html_attr() function gets the destination of the links/images on a page but more specifically extracts the specific attribute of the html element. So, if you are extracting an element with a certain class and want the title, then you can extract just the title by specifying html_attr("class").

```{r}
## The code from below will get all the destinations
## of the links on the page
#
# read_html("https://www.cheese.com/alphabetical") |>
#   html_elements("a") |>
#   html_attr("href") |>
#   head(5)
```

**Part 3:** (Do this alongside Part 4 below.) I used [ChatGPT](https://chat.openai.com/chat) to start the process of scraping cheese information with the following prompt:

> Write R code using the rvest package that allows me to scrape cheese information from cheese.com.

Fully document your process of checking this code. Record any observations you make about where ChatGPT is useful / not useful.

```{r}
#| eval: false
#| label: small-example-of-getting-cheese-info

# Load required libraries
library(rvest)
library(dplyr)

# Define the URL
url <- "https://www.cheese.com/alphabetical"

# Read the HTML content from the webpage
webpage <- read_html(url) ## Comment: also just use read_html("https://www.cheese.com/alphabetical/")

# Extract the cheese names and URLs
## Comment: this code chunk results in an empty character 
cheese_data <- webpage %>%
  html_nodes(".cheese-item") %>%
  html_nodes("a") %>%
  html_attr("href") %>%
  paste0("https://cheese.com", .)

## Comment: this code chunk ALSO results in an empty character 
cheese_names <- webpage %>%
  html_nodes(".cheese-item h3") %>%
  html_text()

# Create a data frame to store the results
## Comment: this dataframe is empty.... because the other objects are
cheese_df <- data.frame(Name = cheese_names,
                        URL = cheese_data,
                        stringsAsFactors = FALSE)

# Print the data frame
print(cheese_df)
```

Observations: - None of the code worked. The `cheese_data` and `cheese_names` objects are empty. - The code was not useful at all. I had to figure out the correct CSS selectors using the selector gadget tool in order to get the cheese names and URLs. - The correct CSS selector for the each cheese's URL is "h3 a". - The correct CSS selector for the cheese names text is "h3".

**Part 4:** Obtain the following information for **all** cheeses in the database:

-   cheese name
-   URL for the cheese's webpage (e.g., <https://www.cheese.com/gouda/>)
-   whether or not the cheese has a picture (e.g., [gouda](https://www.cheese.com/gouda/) has a picture, but [bianco](https://www.cheese.com/bianco/) does not).

To be kind to the website owners, please add a 1 second pause between page queries. (Note that you can view 100 cheeses at a time.)

```{r}
# Setup for the functions
base_url <- "https://www.cheese.com/alphabetical/?per_page=100"
pages <- seq(1, 21, 1)
result <- data.frame()

# Helper function to reduce html_elements() code duplication (thank you Dr. Theobold)
get_text_from_page <- function(page, 
                               css_selector,
                               css_selector_2) {
  page |> 
    html_elements(css_selector) |> 
    html_elements(css_selector_2)
}

# Iterate over all the possible pages
for (page in pages) {
  full_url <- paste0(base_url, "&page=", page)
  webpage <- read_html(full_url)
  
  # Extract the name of the cheese
  name <- get_text_from_page(webpage,
                             "div.product-item",
                             "h3") |> 
    html_text()
  
  # Extract the url of the cheese
  url <- get_text_from_page(webpage,
                            "div.product-item",
                            "h3 a") |> 
    html_attr("href")
  
  # Extract the image class of the cheese
  whether <- get_text_from_page(webpage,
                                "div.product-item",
                                "img") |> 
    html_attr("class")
  
  
  # Put it all together and append to the result dataframe
  result <- rbind(result, data.frame(
    Name = name,
    url = paste0("https://www.cheese.com", url),
    whether = whether
  ))
  
  # Sleep so the website doesn't get angry
  Sys.sleep(1)
}
```

**Part 5:** When you go to a particular cheese's page (like [gouda](https://www.cheese.com/gouda/)), you'll see more detailed information about the cheese. For [**just 10**]{.underline} of the cheeses in the database, obtain the following detailed information:

-   milk information
-   country of origin
-   family
-   type
-   flavour

(Just 10 to avoid overtaxing the website! Continue adding a 1 second pause between page queries.)

**Part 6:** Evaluate the code that you wrote in terms of **efficiency**. To what extent do your function(s) adhere to the **principles for writing good functions**? To what extent are your **functions efficient**? To what extent is your **iteration of these functions efficient**?

There is a little bit of duplication happening when we want specific sections of a webpage so it makes sense to make a separate function to do that for us. The body is also easy to read and there are comments associated with the functions. It is hard to make the functions more efficient than they are right now since it isn't really possible to vectorize the code. There isn't a way to grab multiple sites at once since we are limited by the fact that we can only grab so many pages at once. There is also a significant portion of time just receiving data from the website, so that also adds more time.
