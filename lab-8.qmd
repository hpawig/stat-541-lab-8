---
title: "Lab 8"
subtitle: "Cheese Gromit!"
editor: source
---

> **Goal:** Scrape information from <https://www.cheese.com> to obtain a dataset of characteristics about different cheeses, and gain deeper insight into your coding process. ðŸª¤

**Part 1:** Locate and examine the `robots.txt` file for this website. Summarize what you learn from it.

The robots.txt file for the cheese.com website showed us that any user is authorized to scrape the data from this site. The other other piece of information provided was the sitemap at <https://www.cheese.com/sitemap.xml>. Basically, there are no restrictions for webscraping on this cheese website.

**Part 2:** Learn about the `html_attr()` function from `rvest`. Describe how this function works with a small example.

The html_attr() function gets the destination of the links/images on a page.

```{r}
## The code from below will get all the destinations
## of the links on the page
#
# read_html("https://www.cheese.com/alphabetical") |>
#   html_elements("a") |>
#   html_attr("href") |>
#   head(5)
```

**Part 3:** (Do this alongside Part 4 below.) I used [ChatGPT](https://chat.openai.com/chat) to start the process of scraping cheese information with the following prompt:

> Write R code using the rvest package that allows me to scrape cheese information from cheese.com.

Fully document your process of checking this code. Record any observations you make about where ChatGPT is useful / not useful.

```{r}
#| eval: false
#| label: small-example-of-getting-cheese-info

# Load required libraries
library(rvest)
library(dplyr)

# Define the URL
url <- "https://www.cheese.com/alphabetical"

# Read the HTML content from the webpage
webpage <- read_html(url) ## Comment: also just use read_html("https://www.cheese.com/alphabetical/")

# Extract the cheese names and URLs
## Comment: this code chunk results in an empty character 
cheese_data <- webpage %>%
  html_nodes(".cheese-item") %>%
  html_nodes("a") %>%
  html_attr("href") %>%
  paste0("https://cheese.com", .)

## Comment: this code chunk ALSO results in an empty character 
cheese_names <- webpage %>%
  html_nodes(".cheese-item h3") %>%
  html_text()

# Create a data frame to store the results
## Comment: this dataframe is empty.... because the other objects are
cheese_df <- data.frame(Name = cheese_names,
                        URL = cheese_data,
                        stringsAsFactors = FALSE)

# Print the data frame
print(cheese_df)
```
Observations:
  - None of the code worked. The `cheese_data` and `cheese_names` objects are empty.
  - The code was not useful at all. I had to figure out the correct CSS selectors using the selector gadget tool
  in order to get the cheese names and URLs.
    - The correct CSS selector for the each cheese's URL is "h3 a".
    - The correct CSS selector for the cheese names text is "h3".
  

**Part 4:** Obtain the following information for **all** cheeses in the database:

-   cheese name
-   URL for the cheese's webpage (e.g., <https://www.cheese.com/gouda/>)
-   whether or not the cheese has a picture (e.g., [gouda](https://www.cheese.com/gouda/) has a picture, but [bianco](https://www.cheese.com/bianco/) does not).

To be kind to the website owners, please add a 1 second pause between page queries. (Note that you can view 100 cheeses at a time.)

```{r}
base_url <- "https://www.cheese.com/alphabetical/?per_page=100"
pages <- seq(1, 21, 1)

result <- data.frame()

for (page in pages) {
  full_url <- paste0(base_url, "&page=", page)
  webpage <- read_html(full_url)
  
  name <- webpage |> 
    html_elements("div.product-item") |> 
    html_elements("h3") |> 
    html_text()
  
  url <- webpage |> 
    html_elements("div.product-item") |> 
    html_elements("h3 a") |> 
    html_attr("href")
  
  whether <- webpage |> 
    html_elements("div.product-item") |> 
    html_elements("img") |> 
    html_attr("class")
  
  result <- rbind(result, data.frame(
    Name = name,
    url = paste0("https://www.cheese.com", url),
    whether = whether
  ))
  
  
  Sys.sleep(1)
}
```

**Part 5:** When you go to a particular cheese's page (like [gouda](https://www.cheese.com/gouda/)), you'll see more detailed information about the cheese. For [**just 10**]{.underline} of the cheeses in the database, obtain the following detailed information:

-   milk information
-   country of origin
-   family
-   type
-   flavour

(Just 10 to avoid overtaxing the website! Continue adding a 1 second pause between page queries.)

```{r}
# Helper function to extract certain attributes
get_text_from_page <- function(page, css_selector) {
    
  page %>%
    html_elements(css_selector) %>%
    html_text()
}


# page is a URL
# This function will scrape 1 page and puts the cheese info into a tibble
scrape_page <- function(cheese_url) {
    
    Sys.sleep(1)
    
    # Read the page
    page <- read_html(cheese_url)
    
    # Grab elements from the page
    milk_info <- get_text_from_page(page, ".summary_milk p")
    country_of_origin <- get_text_from_page(page, ".summary_country p")
    family <- get_text_from_page(page, ".summary_family p")
    type <- get_text_from_page(page, ".summary_moisture_and_type p")
    flavour <- get_text_from_page(page, ".summary_taste p")

    
    # Put page elements into a dataframe
    tibble(
        milk = milk_info,
        country_of_origin,
        family, type, flavour
    )
}


# Create a character vector of URLs for the first 5 pages
# scrapes multiple cheese pages with map()
# Cheese vector containing cheeses with a family section
cheese_urls <- result |> 
  filter(Name %in% c(
    "Amul Processed Cheese", "Cheese Curds", "Colby", "Applewood",
    "Vacherin", "Mozzarella di Bufala DOP", "Pecorino Romano",
    "Paesanella Burrata", "Camembert", "Paesanella Cherry Bocconcini"
  )) |> 
  pull(url) 

cheeses_details <- purrr::map(cheese_urls, scrape_page)

# Combine the list of dataframes into one dataframe
df_cheeses <- bind_rows(cheeses_details)

head(df_cheeses)

```


**Part 6:** Evaluate the code that you wrote in terms of **efficiency**. To what extent do your function(s) adhere to the **principles for writing good functions**? To what extent are your **functions efficient**? To what extent is your **iteration of these functions efficient**?
